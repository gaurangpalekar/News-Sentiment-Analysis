{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from nsepython import *\n",
    "import concurrent.futures\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis libraries\n",
    "import nltk\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('https://www.niftyindices.com/IndexConstituent/ind_nifty50list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NIFTY URLS\n",
    "nifty_500_ticker_url = 'https://www.niftyindices.com/IndexConstituent/ind_nifty500list.csv'\n",
    "nifty_500 = pd.read_csv(nifty_500_ticker_url)\n",
    "nifty_500.to_csv('./datasets/NIFTY_500.csv')\n",
    "nifty_200_ticker_url = 'https://www.niftyindices.com/IndexConstituent/ind_nifty200list.csv'\n",
    "nifty_200 = pd.read_csv(nifty_200_ticker_url)\n",
    "nifty_200.to_csv('./datasets/NIFTY_200.csv')\n",
    "nifty_100_ticker_url = 'https://www.niftyindices.com/IndexConstituent/ind_nifty100list.csv'\n",
    "nifty_100 = pd.read_csv(nifty_100_ticker_url)\n",
    "nifty_100.to_csv('./datasets/NIFTY_100.csv')\n",
    "nifty_50_ticker_url = 'https://www.niftyindices.com/IndexConstituent/ind_nifty50list.csv'\n",
    "nifty_50 = pd.read_csv(nifty_50_ticker_url)\n",
    "nifty_50.to_csv('./datasets/NIFTY_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set universe\n",
    "universe = nifty_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV & create a tickers df\n",
    "tickers_df = universe[['Symbol', 'Company Name']]\n",
    "tickers_list = tickers_df['Symbol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# News URL\n",
    "news_url = 'https://ticker.finology.in/company/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch all available data\n",
    "article_data = pd.read_csv('./NIFTY_500_Articles.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# list to store article data\n",
    "article_data = []\n",
    "# list to store tickers for which data is unavailable\n",
    "unavailable_tickers = []\n",
    "# length of companies\n",
    "companies_len = len(tickers_list)\n",
    "tickers_length = companies_len\n",
    "#days_limit = datetime.datetime.now() - datetime.timedelta(days=30) #only 30 days old or newer articles\n",
    "print('Fetching Article data..')\n",
    "for i,ticker in enumerate(tickers_list):\n",
    "    print(i, ticker)\n",
    "    url= '{}/{}'.format(news_url, ticker)\n",
    "    header={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:20.0) Gecko/20100101 Firefox/20.0'}\n",
    "    response = requests.get(url, headers=header)\n",
    "    html = BeautifulSoup(response.content, 'lxml')\n",
    "    news_links = html.select('#newsarticles > a')\n",
    "    if len(news_links) == 0:\n",
    "        print('No news found for {}'.format(ticker))\n",
    "        unavailable_tickers.append(ticker)\n",
    "        continue\n",
    "    # for tickers which are not recognized by finology website, it returns home-page. There's also a news section on homepage. so for unrecognized tickers, this function will scrape general financial news instead of ticker specific news\n",
    "    # var to store article count\n",
    "    ticker_articles_counter = 0\n",
    "    for link in news_links:\n",
    "        art_title = link.find('span', class_='h6').text\n",
    "        #separate date and time from datetime object\n",
    "        date_time_obj = datetime.strptime(link.find('small').text, '%d %b %Y, %I:%M%p')\n",
    "        #if (date_time_obj <= days_limit):\n",
    "        #     continue\n",
    "        art_date = date_time_obj.date().strftime('%Y/%m/%d')\n",
    "        art_time = date_time_obj.time().strftime('%H:%M')\n",
    "        article_data.append([ticker, art_title, art_date, art_time])\n",
    "        ticker_articles_counter += 1\n",
    "    print('No of articles for {} is {}'.format(ticker, ticker_articles_counter))\n",
    "    if(ticker_articles_counter==0):\n",
    "        print('{} added to unavailable list')\n",
    "        unavailable_tickers.append(ticker)  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unavailable_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the loop again, but for the tickers for which news could not be fetched.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a df from the news articles list\n",
    "articles_df = pd.DataFrame(article_data, columns=['Ticker', 'Headline', 'Date', 'Time'])\n",
    "articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of tickers for which news was fetched\n",
    "tickers_list = articles_df['Ticker'].unique()\n",
    "tickers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update original tickers_df to remove tickers for which news was not available\n",
    "tickers_df = tickers_df[tickers_df['Symbol'].isin(tickers_list)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(article_data['Ticker'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis libraries\n",
    "import nltk\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_scores_df = pd.DataFrame(articles_df['Headline'].apply(vader.polarity_scores).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with article details and scores merged.\n",
    "art_scores_df = pd.merge(articles_df, art_scores_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#art_scores_df.to_csv('NIFTY_500_Articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_metadata = pd.read_csv('./ticker_metadata.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Get Company,Sector,Industry,mCap Data\n",
    "ticker_meta = []\n",
    "new_length = len(tickers_list)\n",
    "#new_length = 10\n",
    "print('Fetching ticker meta data')\n",
    "# whether to get only data available in tickers_list or the whole universe bc even if we take whole universe, we will later merge the dfs ticker_meta and art_scores_df\n",
    "# but since we're planning on not to fetch industry and sector data that often, its better to take the universe meta\n",
    "\n",
    "for i, ticker in enumerate(tickers_list):\n",
    "    meta = nse_eq(ticker)\n",
    "    print(i, ticker)\n",
    "    try:\n",
    "        sector = meta['industryInfo']['macro']\n",
    "    except KeyError:\n",
    "        print('{} sector info is not available'.format(ticker))\n",
    "        sector = np.nan\n",
    "        industry = np.nan\n",
    "        mCap = np.nan\n",
    "        companyName = np.nan\n",
    "        ticker_meta.append([ticker, sector, industry, mCap, companyName])\n",
    "        continue\n",
    "    try:\n",
    "        industry = meta['industryInfo']['industry']\n",
    "    except KeyError:\n",
    "        print('{} industry info is not available'.format(ticker))\n",
    "        industry = np.nan\n",
    "        mCap = np.nan\n",
    "        companyName = np.nan\n",
    "        ticker_meta.append([ticker, sector, industry, mCap, companyName])\n",
    "        continue\n",
    "    try:\n",
    "        mCap = round((meta['priceInfo']['previousClose'] * meta['securityInfo']['issuedSize'])/1000000000, 2)\n",
    "    except KeyError:\n",
    "        print('{} mCap data is not available'.format(ticker))\n",
    "        mCap = np.nan\n",
    "        companyName = np.nan\n",
    "        ticker_meta.append([ticker, sector, industry, mCap, companyName])\n",
    "        continue\n",
    "    try:\n",
    "        companyName = meta['info']['companyName']\n",
    "    except KeyError:\n",
    "        print('{} company Name is not available'.format(ticker))\n",
    "        companyName = np.nan\n",
    "        ticker_meta.append([ticker, sector, industry, mCap, companyName])\n",
    "    ticker_meta.append([ticker, sector, industry, mCap, companyName])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xc_indices = pd.read_csv('XC-tickers.csv')\n",
    "xc_indices.columns = ['Ticker', 'XC-Sector', 'XC-Industry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_meta_df = pd.DataFrame(ticker_meta, columns=['Ticker', 'Sector', 'Industry', 'Market Cap', 'Company Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_metadata = pd.merge(ticker_meta_df, xc_indices, on='Ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ticker_metadata.to_csv('ticker_matadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_metadata.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data = article_data.drop(['Unnamed: 0'], axis=1)\n",
    "ticker_metadata = ticker_metadata.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_score_summary = article_data.groupby('Ticker').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_score_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(article_score_summary, ticker_metadata,  on='Ticker', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_scores = article_data.groupby('Ticker').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(ticker_metadata, ticker_scores, on='Ticker', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns = ['Symbol','Macro-Sector','Industry', 'Market Cap (Billion Rs)','Company Name','XC-Sector', 'XC-Industry', 'Negative', 'Neutral', 'Positive', 'Sentiment Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating Plots')\n",
    "fig = px.treemap(\n",
    "    final_df, path=[px.Constant('Nifty 500'), 'XC-Sector', 'XC-Industry', 'Symbol'], values='Market Cap (Billion Rs)', color='Sentiment Score',\n",
    "    hover_data=['Company Name', 'Negative', 'Neutral', 'Positive', 'Sentiment Score'], color_continuous_scale=['#FF0000', \"#000000\", '#00FF00'], color_continuous_midpoint=0\n",
    "    )\n",
    "fig.data[0].customdata = final_df[['Company Name', 'Negative', 'Neutral', 'Positive', 'Sentiment Score']]\n",
    "fig.data[0].texttemplate = \"%{label}<br>%{customdata[4]}\"\n",
    "fig.update_traces(textposition=\"middle center\")\n",
    "fig.update_layout(margin = dict(t=30, l=10, r=10, b=10), font_size=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tickers = ['3MINDIA', 'APLAPOLLO', 'ABBOTINDIA', 'ABSLAMC', 'ALKYLAMINE', 'AMARAJABAT', 'AMBER', 'APOLLOTYRE', 'APTUS', 'ASTRAZEN', 'BEML', 'BAJAJELEC', 'BALKRISIND', 'BATAINDIA', 'BAYERCROP', 'BERGEPAINT', 'BHARATRAS', 'BLUEDART', 'BRITANNIA', 'MAPMYINDIA', 'CESC', 'CAMPUS', 'CEATLTD', 'CDSL', 'CENTURYPLY', 'CHOLAHLDNG', 'COLPAL', 'CUMMINSIND', 'DEVYANI', 'DIVISLAB', 'DIXON', 'EIHOTEL', 'EPL', 'EMAMILTD', 'NYKAA', 'FACT', 'FINEORG', 'GAIL', 'GICRE', 'GODFRYPHLP', 'GUJALKALI', 'GUJGASLTD', 'GSPL', 'HLEGLAS', 'HIKAL', 'HGS', 'HAL', 'HONAUT', 'HUDCO', 'IFBIND', 'INDIACEM', 'IBULHSGFIN', 'IRFC', 'IGL', 'IPCALAB', 'JBCHEPHARM', 'KNRCON', 'KPITTECH', 'KANSAINER', 'L&TFH', 'LAXMIMACH', 'LUXIND', 'MRF', 'MGL', 'M&MFIN', 'M&M', 'MAHLOG', 'MAZDOCK', 'MEDPLUS', 'METROPOLIS', 'MSUMI', 'MUTHOOTFIN', 'NLCINDIA', 'NATIONALUM', 'NAVINFLUOR', 'NESTLEIND', 'NAM-INDIA', 'NUVOCO', 'OBEROIRLTY', 'OIL', 'OLECTRA', 'PCBL', 'PATANJALI', 'PFIZER', 'POLYPLEX', 'PFC', 'PRESTIGE', 'PRINCEPIPE', 'PGHL', 'RAIN', 'RAINBOW', 'RATNAMANI', 'REDINGTON', 'RBA', 'SIS', 'SRF', 'SHILPAMED', 'SAIL', 'SUMICHEM', 'SUNTV', 'SUNDARMFIN', 'SUPREMEIND', 'SYMPHONY', 'TCNSBRANDS', 'TTML', 'TEJASNET', 'THERMAX', 'THYROCARE', 'TORNTPHARM', 'TIINDIA', 'UPL', 'UTIAMC', 'VIPIND', 'VAIBHAVGBL', 'VBL', 'MANYAVAR', 'VIJAYA', 'VOLTAS', 'WHIRLPOOL', 'WOCKPHARMA', 'ZFCVINDIA', 'ZEEL', 'ZENSARTECH', 'ZOMATO', 'ECLERX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_url = 'https://ticker.finology.in/company/'\n",
    "# list to store article data\n",
    "article_data = []\n",
    "unavailable_tickers = []\n",
    "# list to store tickers for which data is unavailable\n",
    "# length of companies\n",
    "companies_len = len(test_tickers)\n",
    "tickers_length = companies_len\n",
    "#days_limit = datetime.datetime.now() - datetime.timedelta(days=30) #only 30 days old or newer articles\n",
    "print('Fetching Article data..')\n",
    "for i in range(tickers_length):\n",
    "    print(i, test_tickers[i])\n",
    "    url= '{}/{}'.format(news_url, test_tickers[i])\n",
    "    header={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:20.0) Gecko/20100101 Firefox/20.0'}\n",
    "    response = requests.get(url, headers=header)\n",
    "    html = BeautifulSoup(response.content, 'lxml')\n",
    "    news_links = html.select('#newsarticles > a')\n",
    "    if len(news_links) == 0:\n",
    "        print('No news found for {}'.format(test_tickers[i]))\n",
    "        unavailable_tickers.append(test_tickers[i])\n",
    "        continue\n",
    "    # for tickers which are not recognized by finology website, it returns home-page. There's also a news section on homepage. so for unrecognized tickers, this function will scrape general financial news instead of ticker specific news\n",
    "    # var to store article count\n",
    "    ticker_articles_counter = 0\n",
    "    for link in news_links:\n",
    "        art_title = link.find('span', class_='h6').text\n",
    "        #separate date and time from datetime object\n",
    "        date_time_obj = datetime.datetime.strptime(link.find('small').text, '%d %b %Y, %I:%M%p')\n",
    "        #if (date_time_obj <= days_limit):\n",
    "        #     continue\n",
    "        art_date = date_time_obj.date().strftime('%Y/%m/%d')\n",
    "        art_time = date_time_obj.time().strftime('%H:%M')\n",
    "        article_data.append([test_tickers[i], art_title, art_date, art_time])\n",
    "        ticker_articles_counter += 1\n",
    "    if(ticker_articles_counter==0):\n",
    "        print('Article length = 0 for {}'.format(test_tickers[i]))\n",
    "        unavailable_tickers.append(tickers_list[i])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(article_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= 'https://ticker.finology.in/company/EMAMILTD'\n",
    "header={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:20.0) Gecko/20100101 Firefox/20.0'}\n",
    "response = requests.get(url, headers=header)\n",
    "html = BeautifulSoup(response.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_links = html.select('#newsarticles > a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "236178af6f648e00519e077836b3ac634bd3def9b921796a679cc929c64b686e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
